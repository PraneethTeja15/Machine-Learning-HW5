# Machine-Learning-HW5
This project implements scaled dot-product attention in NumPy and a simplified Transformer encoder block in PyTorch, including multi-head self-attention, feed-forward layers, residual connections, and layer normalization. The code verifies correct dimensions for typical input batches.
